{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import nltk\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"data/train.tsv\", sep=\"\\t\") # 训练集\n",
    "test_data = pd.read_csv(\"data/test.tsv\", sep=\"\\t\") # 测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2    156063        8545                                                 An\n",
       "3    156064        8545  intermittently pleasing but mostly routine effort\n",
       "4    156065        8545         intermittently pleasing but mostly routine"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集中phrases数量: 156060. 训练集中sentences 数量: 8529.\n",
      "测试集中phrases数量: 66292. 测试集中sentences 数量: 3310.\n"
     ]
    }
   ],
   "source": [
    "print('训练集中phrases数量: {}. 训练集中sentences 数量: {}.'.format(train_data.shape[0], len(train_data.SentenceId.unique())))\n",
    "print('测试集中phrases数量: {}. 测试集中sentences 数量: {}.'.format(test_data.shape[0], len(test_data.SentenceId.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集train_data中平均每个sentences的phrases数量：18.\n",
      "测试集test_data中平均每个sentences的phrases数量：20.\n"
     ]
    }
   ],
   "source": [
    "# 按照SentenceId进行分组\n",
    "print('训练集train_data中平均每个sentences的phrases数量：{0:.0f}.'.format(train_data.groupby('SentenceId')['Phrase'].count().mean()))\n",
    "print('测试集test_data中平均每个sentences的phrases数量：{0:.0f}.'.format(test_data.groupby('SentenceId')['Phrase'].count().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A positively thrilling combination of ethnography and all the intrigue , betrayal , deceit and murder of a Shakespearean tragedy or a juicy soap opera . A positively thrilling combination of ethnography and all the intrigue , betrayal , deceit and murder of a Shakespearean tragedy or a juicy soap opera A positively thrilling combination of ethnography and all the intrigue , betrayal , deceit and murder A positively thrilling combination positively thrilling combination positively thrilling combination thrilling combination of ethnography and all the intrigue , betrayal , deceit and murder ethnography and all the intrigue , betrayal , deceit and murder ethnography and ethnography all the intrigue , betrayal , deceit and murder all the intrigue , betrayal , deceit and murder intrigue , betrayal , deceit and murder intrigue , betrayal , deceit and murder betrayal , deceit and murder betrayal , deceit and murder deceit and murder deceit and deceit murder of a Shakespearean tragedy or a juicy soap opera a Shakespearean tragedy or a juicy soap opera a Shakespearean tragedy or a Shakespearean tragedy Shakespearean tragedy Shakespearean tragedy or a juicy soap opera juicy soap opera juicy soap opera soap opera\n"
     ]
    }
   ],
   "source": [
    "# 查看positive的短语中不同三元组的出现频数\n",
    "text = ' '.join(train_data['Phrase'].loc[train_data.SentenceId == 4])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .', 'A series of escapades demonstrating the adage that what is good for the goose', 'A series', 'A', 'series', 'of escapades demonstrating the adage that what is good for the goose', 'of', 'escapades demonstrating the adage that what is good for the goose', 'escapades', 'demonstrating the adage that what is good for the goose', 'demonstrating the adage', 'demonstrating', 'the adage', 'the', 'adage', 'that what is good for the goose', 'that', 'what is good for the goose', 'what', 'is good for the goose', 'is', 'good for the goose', 'good', 'for the goose', 'for', 'the goose', 'goose', 'is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .', 'is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story', 'is also', 'also', 'good for the gander , some of which occasionally amuses but none of which amounts to much of a story', 'for the gander , some of which occasionally amuses but none of which amounts to much of a story', 'the gander , some of which occasionally amuses but none of which amounts to much of a story', 'the gander ,', 'the gander', 'gander', ',', 'some of which occasionally amuses but none of which amounts to much of a story', 'some of which', 'some', 'of which', 'which', 'occasionally amuses but none of which amounts to much of a story', 'occasionally', 'amuses but none of which amounts to much of a story', 'amuses', 'but none of which amounts to much of a story', 'but', 'none of which amounts to much of a story']\n"
     ]
    }
   ],
   "source": [
    "# 将所有Phrase存入一个列表\n",
    "full_text = list(train_data['Phrase']) + list(test_data['Phrase'])\n",
    "print(full_text[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 向量化文本，将文本转换成序列\n",
    "tk = Tokenizer(lower = True, filters='') # 初始化分词器\n",
    "tk.fit_on_texts(full_text) # 对整个文本进行拟合\n",
    "train_tokenized = tk.texts_to_sequences(train_data['Phrase']) # 将训练集文本进行序列化\n",
    "test_tokenized = tk.texts_to_sequences(test_data['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     3   308     4 18264  7830     1  8332    11    55    10    51\n",
      "     15     1  4669    10   185    51    15     1 14845     2    64     4\n",
      "     91   560 13389    21   610     4    91  2702     6    54     4     3\n",
      "     44     7]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      3   308     4 18264  7830     1  8332    11    55    10    51    15\n",
      "      1  4669]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      3   308]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     3]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0   308]]\n"
     ]
    }
   ],
   "source": [
    "# 进行补长处理\n",
    "max_len = 50\n",
    "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "print(X_train[:5])\n",
    "X_test = pad_sequences(test_tokenized, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据预训练的词向量文件初始化词向量矩阵（与之前写过的ChineseNER_keras中的keras_lstm_test中的做法类似），先根据预训练的词向量文件获取字典embedding_index，键值对为word-vector，然后根据word_index（词与索引之间的映射字典）与embedding_index得到字典embedding_matrix，键值对为index-vector。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great: [-0.013786   0.38216    0.53236    0.15261   -0.29694   -0.20558\n",
      " -0.41846   -0.58437   -0.77355   -0.87866   -0.37858   -0.18516\n",
      " -0.128     -0.20584   -0.22925   -0.42599    0.3725     0.26077\n",
      " -1.0702     0.62916   -0.091469   0.70348   -0.4973    -0.77691\n",
      "  0.66045    0.09465   -0.44893    0.018917   0.33146   -0.35022\n",
      " -0.35789    0.030313   0.22253   -0.23236   -0.19719   -0.0053125\n",
      " -0.25848    0.58081   -0.10705   -0.17845   -0.16206    0.087086\n",
      "  0.63029   -0.76649    0.51619    0.14073    1.019     -0.43136\n",
      "  0.46138   -0.43585   -0.47568    0.19226    0.36065    0.78987\n",
      "  0.088945  -2.7814    -0.15366    0.01015    1.1798     0.15168\n",
      " -0.050112   1.2626    -0.77527    0.36031    0.95761   -0.11385\n",
      "  0.28035   -0.02591    0.31246   -0.15424    0.3778    -0.13599\n",
      "  0.2946    -0.31579    0.42943    0.086969   0.019169  -0.27242\n",
      " -0.31696    0.37327    0.61997    0.13889    0.17188    0.30363\n",
      " -1.2776     0.044423  -0.52736   -0.88536   -0.19428   -0.61947\n",
      " -0.10146   -0.26301   -0.061707   0.36627   -0.95223   -0.39346\n",
      " -0.69183   -1.0426     0.28855    0.63056  ]\n"
     ]
    }
   ],
   "source": [
    "embedding_path = \"data/glove.6B.100d.txt\"\n",
    "embed_size = 100\n",
    "max_features = 20000 # embedding_matrix的维度，即需要根据预训练词向量进行初始化的单词数量\n",
    "def get_vector(word, *arr):\n",
    "    return word, np.asarray(arr, dtype=\"float32\")\n",
    "embedding_index = dict(get_vector(*word_vector.strip().split(\" \")) for word_vector in open(embedding_path, encoding=\"utf-8\"))\n",
    "print(\"great:\", embedding_index[\"great\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great: 134\n",
      "[-0.013786    0.38216001  0.53236002  0.15261    -0.29694    -0.20558\n",
      " -0.41846001 -0.58437002 -0.77354997 -0.87866002 -0.37858    -0.18516\n",
      " -0.12800001 -0.20584001 -0.22925    -0.42598999  0.3725      0.26076999\n",
      " -1.07019997  0.62915999 -0.091469    0.70348001 -0.4973     -0.77691001\n",
      "  0.66044998  0.09465    -0.44893     0.018917    0.33146    -0.35021999\n",
      " -0.35789001  0.030313    0.22253001 -0.23236001 -0.19719    -0.0053125\n",
      " -0.25848001  0.58081001 -0.10705    -0.17845    -0.16205999  0.087086\n",
      "  0.63028997 -0.76648998  0.51618999  0.14072999  1.01900005 -0.43136001\n",
      "  0.46138    -0.43584999 -0.47567999  0.19226     0.36065     0.78987002\n",
      "  0.088945   -2.78139997 -0.15366     0.01015     1.17980003  0.15167999\n",
      " -0.050112    1.26259995 -0.77526999  0.36030999  0.95761001 -0.11385\n",
      "  0.28035    -0.02591     0.31246001 -0.15424     0.37779999 -0.13598999\n",
      "  0.29460001 -0.31579     0.42943001  0.086969    0.019169   -0.27241999\n",
      " -0.31696001  0.37327     0.61997002  0.13889     0.17188001  0.30362999\n",
      " -1.27760005  0.044423   -0.52736002 -0.88536    -0.19428    -0.61947\n",
      " -0.10146    -0.26301    -0.061707    0.36627001 -0.95222998 -0.39346001\n",
      " -0.69182998 -1.04260004  0.28854999  0.63055998]\n"
     ]
    }
   ],
   "source": [
    "word_index = tk.word_index\n",
    "print(\"great:\", word_index[\"great\"])\n",
    "num_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words+1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    # word的索引是根据词频进行排列的，所以索引大于预设的max_features时，不进行初始化\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    #get(word)替代[i],遇到key不存在不会报异常，而是直接返回None\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(embedding_matrix[word_index[\"great\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对label进行one-hot处理\n",
    "y = train_data[\"Sentiment\"]\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot = OneHotEncoder(sparse=False)\n",
    "y_onehot = onehot.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\n",
    "file_path = \"best_model.hdf5\"\n",
    "check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                              save_best_only = True, mode = \"min\")\n",
    "early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "\n",
    "def build_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(dr)(x)\n",
    "\n",
    "    x_gru = Bidirectional(GRU(units, return_sequences = True))(x1)\n",
    "    x1 = Conv1D(32, kernel_size=3, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(32, kernel_size=2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_gru = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    x_lstm = Bidirectional(LSTM(units, return_sequences = True))(x1)\n",
    "    x1 = Conv1D(32, kernel_size=3, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(32, kernel_size=2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    # keras的concatenate类似于TensorFlow的concat，将卷积层的输出拼接再一起作为最终的输出\n",
    "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n",
    "                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(Dense(128,activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(Dense(100,activation='relu') (x))\n",
    "    x = Dense(5, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    print(\"compile end\")\n",
    "    history = model.fit(X_train, y_onehot, batch_size = 128, epochs = 15, validation_split=0.1, \n",
    "                        verbose = 1, callbacks = [check_point, early_stop])\n",
    "    print(\"train end\")\n",
    "    model = load_model(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile end\n",
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/15\n",
      "140416/140454 [============================>.] - ETA: 0s - loss: 0.4222 - acc: 0.8142Epoch 00000: val_loss improved from inf to 0.36360, saving model to best_model.hdf5\n",
      "140454/140454 [==============================] - 196s - loss: 0.4222 - acc: 0.8142 - val_loss: 0.3636 - val_acc: 0.8415\n",
      "Epoch 2/15\n",
      "140416/140454 [============================>.] - ETA: 0s - loss: 0.3722 - acc: 0.8375Epoch 00001: val_loss improved from 0.36360 to 0.35243, saving model to best_model.hdf5\n",
      "140454/140454 [==============================] - 193s - loss: 0.3721 - acc: 0.8375 - val_loss: 0.3524 - val_acc: 0.8428\n",
      "Epoch 3/15\n",
      "140416/140454 [============================>.] - ETA: 0s - loss: 0.3610 - acc: 0.8411Epoch 00002: val_loss improved from 0.35243 to 0.35021, saving model to best_model.hdf5\n",
      "140454/140454 [==============================] - 191s - loss: 0.3610 - acc: 0.8412 - val_loss: 0.3502 - val_acc: 0.8425\n",
      "Epoch 4/15\n",
      "140416/140454 [============================>.] - ETA: 0s - loss: 0.3544 - acc: 0.8436Epoch 00003: val_loss improved from 0.35021 to 0.34193, saving model to best_model.hdf5\n",
      "140454/140454 [==============================] - 191s - loss: 0.3544 - acc: 0.8436 - val_loss: 0.3419 - val_acc: 0.8448\n",
      "Epoch 5/15\n",
      "140416/140454 [============================>.] - ETA: 0s - loss: 0.3497 - acc: 0.8450Epoch 00004: val_loss did not improve\n",
      "140454/140454 [==============================] - 191s - loss: 0.3497 - acc: 0.8450 - val_loss: 0.3447 - val_acc: 0.8444\n",
      "Epoch 6/15\n",
      "140416/140454 [============================>.] - ETA: 0s - loss: 0.3460 - acc: 0.8462Epoch 00005: val_loss improved from 0.34193 to 0.33936, saving model to best_model.hdf5\n",
      "140454/140454 [==============================] - 188s - loss: 0.3460 - acc: 0.8462 - val_loss: 0.3394 - val_acc: 0.8457\n",
      "Epoch 7/15\n",
      "140416/140454 [============================>.] - ETA: 0s - loss: 0.3430 - acc: 0.8471Epoch 00006: val_loss improved from 0.33936 to 0.33590, saving model to best_model.hdf5\n",
      "140454/140454 [==============================] - 189s - loss: 0.3430 - acc: 0.8471 - val_loss: 0.3359 - val_acc: 0.8460\n",
      "Epoch 8/15\n",
      "140416/140454 [============================>.] - ETA: 0s - loss: 0.3402 - acc: 0.8477Epoch 00007: val_loss improved from 0.33590 to 0.33464, saving model to best_model.hdf5\n",
      "140454/140454 [==============================] - 190s - loss: 0.3402 - acc: 0.8477 - val_loss: 0.3346 - val_acc: 0.8467\n",
      "Epoch 9/15\n",
      "140416/140454 [============================>.] - ETA: 0s - loss: 0.3383 - acc: 0.8490Epoch 00008: val_loss improved from 0.33464 to 0.33362, saving model to best_model.hdf5\n",
      "140454/140454 [==============================] - 189s - loss: 0.3383 - acc: 0.8490 - val_loss: 0.3336 - val_acc: 0.8473\n",
      "Epoch 10/15\n",
      " 80896/140454 [================>.............] - ETA: 80s - loss: 0.3356 - acc: 0.8492"
     ]
    }
   ],
   "source": [
    "model = build_model(lr = 1e-4, lr_d = 0, units = 128, dr = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-55e650c6c94b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test, batch_size = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
